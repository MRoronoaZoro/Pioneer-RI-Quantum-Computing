# FILE: run_ctqw.py
import os
import json
import csv
import time
import math
import numpy as np
import pennylane as qml
import random
from collections import deque
from scipy.linalg import expm

# --- å…¨å±€é…ç½® ---
ROOT_PATH = r'C:\Users\59415\Desktop\Pioneer RI\Research\Code'
DATASETS_PATH = os.path.join(ROOT_PATH, 'datasets')
RESULTS_PATH = os.path.join(ROOT_PATH, 'results')
ALGORITHM_NAME = 'CTQW'
MAX_STEPS = 100  # æœ€å¤§æ—¶é—´æ­¥ï¼ˆt ä»¥æ•´æ•°æ­¥å¾ªç¯ï¼‰
SHOTS = 10000  # æœç´¢æ¨¡å¼çš„é‡‡æ ·æ¬¡æ•°
MAX_QUBITS = 15  # é™åˆ¶ä»¥é¿å…å†…å­˜é—®é¢˜
EPSILON = 1e-10  # æ¦‚ç‡é˜ˆå€¼ç”¨äºåˆ¤æ–­èŠ‚ç‚¹è¢«"è®¿é—®"

def extract_structure_params(graph_data):
    """æå–ç»“æ„å‚æ•°ï¼ˆä¸DTQWç›¸åŒï¼‰"""
    graph_id = graph_data.get('graph_id', '').lower()
    structure_info = {}
    if 'branch' in graph_id and 'height' in graph_id:
        parts = graph_id.split('_')
        for part in parts:
            if part.startswith('branch'):
                structure_info['branch'] = int(part.replace('branch', ''))
            elif part.startswith('height'):
                structure_info['height'] = int(part.replace('height', ''))
    elif 'grid' in graph_id:
        parts = graph_id.replace('grid_', '').split('_')
        for part in parts:
            if 'x' in part:
                try:
                    dims = part.split('x')
                    structure_info['rows'] = int(dims[0])
                    structure_info['cols'] = int(dims[1])
                except:
                    pass
    elif 'hypercube' in graph_id:
        parts = graph_id.split('_')
        for part in parts:
            if part.startswith('dim'):
                structure_info['dimension'] = int(part.replace('dim', ''))
    return structure_info

def get_target_node(neighbors, graph_id, structure_type):
    """è·å–åˆé€‚çš„ç›®æ ‡èŠ‚ç‚¹ï¼ˆä¸DTQWç›¸åŒï¼‰"""
    num_nodes = len(neighbors)
    if structure_type == 'Tree':
        # å¯»æ‰¾å¶å­èŠ‚ç‚¹
        for node in range(num_nodes - 1, -1, -1):
            if len(neighbors[node]) == 1 and node != 0:  # å¶å­èŠ‚ç‚¹ä½†ä¸æ˜¯æ ¹
                return node
        return num_nodes - 1
    elif structure_type == 'Grid':
        return num_nodes - 1  # å¯¹è§’æœ€è¿œç‚¹
    elif structure_type == 'Hypercube':
        return num_nodes - 1  # ç›¸å¯¹é¡¶ç‚¹
    else:
        # SmallWorld, GluedTree
        return fast_farthest_node(neighbors, 0)

def fast_farthest_node(neighbors, start_node):
    """å¿«é€ŸBFSæ‰¾æœ€è¿œèŠ‚ç‚¹ï¼ˆä¸DTQWç›¸åŒï¼‰"""
    if not neighbors: return 0
    visited = {start_node}
    queue = [(start_node, 0)]
    farthest_node = start_node
    max_depth = 0
    head = 0
    while head < len(queue):
        node, depth = queue[head]
        head += 1
        if depth > max_depth:
            max_depth = depth
            farthest_node = node
        if depth > 8: continue
        for neighbor in neighbors[node]:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, depth + 1))
    return farthest_node

def calculate_graph_diameter(neighbors, start_node, target_node):
    """ä½¿ç”¨BFSè®¡ç®—ä»startåˆ°targetçš„è·ç¦»ï¼ˆä½œä¸ºç›´å¾„ä¼°è®¡ï¼Œä¸DTQWç›¸åŒï¼‰"""
    if not neighbors: return 1
    visited = set()
    queue = deque([(start_node, 0)])
    visited.add(start_node)
    while queue:
        node, dist = queue.popleft()
        if node == target_node:
            return dist
        for neighbor in neighbors[node]:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, dist + 1))
    return 1  # fallback if not connected

def calculate_correct_theoretical_steps(graph_data, structure_type, metric_type="hitting_time"):
    """
    åŸºäºCTQWç†è®ºçš„å‡†ç¡®æ—¶é—´ t è®¡ç®—ï¼ˆæµ®ç‚¹ï¼Œå•ä½æ—¶é—´æ­¥ï¼‰
    - hitting_time:  (Ï€/2) * sqrt(N) (æ­£ç¡®çš„CTQWæœç´¢æ—¶é—´)
    - cover_time: N * log(N) (approximate upper bound)
    """
    num_nodes = len(graph_data['neighbors'])
    structure_info = extract_structure_params(graph_data)
    if num_nodes <= 1:
        return 1.0
    if metric_type == "hitting_time":
        # æ­£ç¡®çš„CTQWæœç´¢æ—¶é—´ï¼š(Ï€/2) * sqrt(N)
        theoretical_t = (math.pi / 2) * math.sqrt(num_nodes)
        print(f"        Theory t = (pi/2) * sqrt({num_nodes}) â‰ˆ {theoretical_t:.2f}")
    else:  # cover_time
        theoretical_t = 0
    return max(theoretical_t, 1.0)

# 1. ä¿®æ­£çš„å“ˆå¯†é¡¿é‡æ„å»º
def build_hamiltonian(num_nodes, neighbors, is_search=False, target_node=None):
    """
    æ„å»ºCTQWçš„å“ˆå¯†é¡¿é‡çŸ©é˜µ H (N x N)
    - è¦†ç›–æ¨¡å¼: H = -A (è´Ÿé‚»æ¥çŸ©é˜µï¼Œæ ‡å‡†CTQW)
    - æœç´¢æ¨¡å¼: H = -A - Ï‰|target><target| (æ·»åŠ Oracleé¡¹)
    """
    # æ„å»ºé‚»æ¥çŸ©é˜µï¼ˆç¡®ä¿å¯¹ç§°æ€§ï¼‰
    A = np.zeros((num_nodes, num_nodes), dtype=complex)
    for i in range(num_nodes):
        for j in neighbors[i]:
            A[i, j] = 1.0  # æ— å‘å›¾ï¼Œå¯¹ç§°
            A[j, i] = 1.0  # ç¡®ä¿å¯¹ç§°
    
    # åŸºç¡€å“ˆå¯†é¡¿é‡ï¼šH = -Aï¼ˆæ ‡å‡†CTQWå½¢å¼ï¼‰
    H = -A
    
    if is_search and target_node is not None:
        # æœç´¢æ¨¡å¼ï¼šæ·»åŠ Oracleé¡¹ H = -A - Ï‰|target><target|
        # åŠ¨æ€è°ƒæ•´Oracleå¼ºåº¦ï¼šåŸºäºå›¾çš„å¹³å‡åº¦æ•°
        avg_degree = np.sum(A) / (2 * num_nodes)  # æ¯æ¡è¾¹è¢«è®¡ç®—ä¸¤æ¬¡
        oracle_strength = max(1.0, avg_degree)  # è‡³å°‘ä¸º1ï¼Œéšè¿é€šæ€§å¢åŠ 
        H[target_node, target_node] -= oracle_strength
        print(f"        æ·»åŠ Oracleé¡¹åˆ°èŠ‚ç‚¹ {target_node}ï¼Œå¼ºåº¦: {oracle_strength:.2f} (å¹³å‡åº¦æ•°: {avg_degree:.2f})")
    
    return H

def pad_hamiltonian(H, num_qubits):
    """
    å°† N x N çš„ H å¡«å……åˆ° 2^q x 2^q (q = num_qubits)ï¼Œæ— æ•ˆçŠ¶æ€æ— è‡ªç¯ï¼ˆå¯¹è§’0ï¼‰
    """
    dim = 1 << num_qubits
    H_pad = np.zeros((dim, dim), dtype=complex)
    N = H.shape[0]
    H_pad[:N, :N] = H
    return H_pad

# 2. PennyLaneé‡å­ç”µè·¯å®šä¹‰
def create_ctqw_circuit(
        neighbors,
        target_t,
        start_node,
        num_pos_qubits,
        target_node=None,
        with_shots: bool = False,
        is_search: bool = False,
):
    """
    æ„å»ºCTQWç”µè·¯
    - åˆå§‹åŒ–: æœç´¢æ¨¡å¼ç”¨å‡åŒ€å åŠ ï¼›è¦†ç›–æ¨¡å¼ç”¨æœ¬åœ°åŒ–çŠ¶æ€
    - æ¼”åŒ–: U(t) = exp(-i H t)ï¼Œä½¿ç”¨ QubitUnitary (expm)
    - æµ‹é‡: ä»…ä½ç½®æ¯”ç‰¹çš„æ ·æœ¬æˆ–æ¦‚ç‡
    """
    num_nodes = len(neighbors)
    if num_nodes == 0:
        raise ValueError("Graph has no nodes.")
    total_qubits = num_pos_qubits
    pos_qubits = list(range(total_qubits))
    
    # æ„å»ºå“ˆå¯†é¡¿é‡å¹¶å¡«å……
    H = build_hamiltonian(num_nodes, neighbors, is_search=is_search, target_node=target_node)
    H_pad = pad_hamiltonian(H, total_qubits)
    
    # æ—¶é—´æ¼”åŒ–ç®—å­ U(t) = exp(-i H t)
    U_t = expm(-1j * H_pad * target_t)
    
    # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
    if np.any(np.isnan(U_t)) or np.any(np.isinf(U_t)):
        print(f"        è­¦å‘Šï¼šæ—¶é—´æ¼”åŒ–ç®—å­åŒ…å«NaNæˆ–Infå€¼ï¼Œt={target_t}")
        U_t = np.eye(H_pad.shape[0], dtype=complex)  # å›é€€åˆ°å•ä½çŸ©é˜µ
    
    dev = qml.device("lightning.qubit", wires=total_qubits, shots=SHOTS if with_shots else None)
    
    @qml.qnode(dev)
    def ctqw_circuit():
        # ========== ä¿®æ­£çš„åˆå§‹åŒ– ==========
        if is_search:
            # æœç´¢æ¨¡å¼: å‡åŒ€å åŠ æ‰€æœ‰æœ‰æ•ˆèŠ‚ç‚¹çŠ¶æ€
            for q in pos_qubits:
                qml.Hadamard(wires=q)
            
            # å¦‚æœèŠ‚ç‚¹æ•°ä¸æ˜¯2çš„å¹‚ï¼Œéœ€è¦å½’ä¸€åŒ–è°ƒæ•´
            if num_nodes < (1 << num_pos_qubits):
                print(f"        æœç´¢æ¨¡å¼ï¼šå‡åŒ€å åŠ  {num_nodes} ä¸ªæœ‰æ•ˆèŠ‚ç‚¹çŠ¶æ€")
        else:
            # è¦†ç›–æ¨¡å¼: æœ¬åœ°åŒ–åœ¨ start_node
            # ä½¿ç”¨å¤§ç«¯åºç¼–ç ï¼šæœ€é«˜ä½å¯¹åº” qubit[0]
            bin_start = bin(start_node)[2:].zfill(num_pos_qubits)
            for i in range(num_pos_qubits):
                if bin_start[i] == '1':  # å¤§ç«¯åºï¼šbin_start[0] å¯¹åº”æœ€é«˜ä½
                    qml.PauliX(wires=pos_qubits[i])
            print(f"        è¦†ç›–æ¨¡å¼ï¼šåˆå§‹åŒ–èŠ‚ç‚¹ {start_node} (äºŒè¿›åˆ¶: {bin_start})")
        
        # ========== CTQW EVOLUTION ==========
        qml.QubitUnitary(U_t, wires=pos_qubits)
        
        # ========== MEASUREMENT ==========
        if with_shots:
            return qml.sample(wires=pos_qubits)
        else:
            return qml.probs(wires=pos_qubits)
    
    return ctqw_circuit

def calculate_coverage_steps(
    neighbors,
    start_node,
    theoretical_cover_t,
    num_pos_qubits,
    repetitions: int = 10,
):
    """
    è¿è¡Œè¦†ç›–æ¨¡å¼CTQWï¼ˆæ— æ ‡è®°ï¼‰ï¼Œé‡å¤repetitionsæ¬¡ã€‚
    æ¯æ¬¡ä»t=1é€’å¢ï¼Œç›´åˆ°è‡³å°‘90%èŠ‚ç‚¹æ¦‚ç‡ > EPSILONã€‚
    è¿”å›å¹³å‡æœ€å°tå’Œå¹³å‡wall-clockæ—¶é—´ã€‚
    """
    num_nodes = len(neighbors)
    if num_nodes == 0:
        return 1.0, 0.0
    target_cnt = int(np.ceil(num_nodes * 0.9))
    t_list = []
    time_list = []
    for rep in range(repetitions):
        visited = set()
        start = time.time()
        found = False
        for steps in range(1, MAX_STEPS + 1):
            t = float(steps)  # t = steps (ç¦»æ•£åŒ–)
            try:
                circ = create_ctqw_circuit(
                    neighbors,
                    t,
                    start_node,
                    num_pos_qubits,
                    target_node=None,
                    with_shots=False,
                    is_search=False,
                )
                prob_vec = circ()[:num_nodes]  # æˆªæ–­å¡«å……
                newly = {i for i, p in enumerate(prob_vec) if p > EPSILON}
                visited.update(newly)
                if len(visited) >= target_cnt:
                    t_list.append(t)
                    found = True
                    break
            except Exception as exc:
                print(f"        Coverage t {t} failed (rep {rep+1}): {exc}")
                continue
        elapsed = time.time() - start
        time_list.append(elapsed)
        if not found:
            t_list.append(float(MAX_STEPS))
            print(f"        Rep {rep+1}: coverage not reached within {MAX_STEPS} steps.")
    avg_t = float(np.mean(t_list))
    avg_time = float(np.mean(time_list))
    print(f"        Coverage avg t over {repetitions} runs: {avg_t:.2f}")
    print(f"        Coverage avg time over {repetitions} runs: {avg_time:.3f}s")
    return avg_t, avg_time

def calculate_search_probability(
    neighbors,
    start_node,
    target_node,
    theoretical_t,
    num_pos_qubits,
):
    """
    è¿è¡Œæœç´¢æ¨¡å¼CTQWï¼ˆæ ‡è®°ç›®æ ‡ï¼‰ï¼Œæ‰«æç†è®ºæ—¶é—´é™„è¿‘çš„æ—¶é—´ç‚¹æ‰¾åˆ°æœ€ä¼˜æœç´¢æ¦‚ç‡ã€‚
    è¿”å›(æœ€å¤§å‡»ä¸­æ¦‚ç‡, wall-clockè¿è¡Œæ—¶é—´, æœ€ä¼˜æ—¶é—´)ã€‚
    """
    try:
        t0 = time.time()
        
        # æ—¶é—´æ‰«æèŒƒå›´ï¼šç†è®ºæ—¶é—´çš„ Â±20%ï¼Œåˆ†5ä¸ªç‚¹
        time_points = [
            theoretical_t * 0.8,
            theoretical_t * 0.9,
            theoretical_t,
            theoretical_t * 1.1,
            theoretical_t * 1.2
        ]
        
        best_prob = 0.0
        best_time = theoretical_t
        best_hits = 0
        best_valid_size = 0
        
        for t in time_points:
            circ = create_ctqw_circuit(
                neighbors,
                t,
                start_node,
                num_pos_qubits,
                target_node=target_node,
                with_shots=True,
                is_search=True,
            )
            # shape = (shots, num_pos_qubits)
            samples = circ()
            
            # ä¿®æ­£ï¼šä½¿ç”¨å¤§ç«¯åºè§£ç æ ·æœ¬åˆ°èŠ‚ç‚¹ç´¢å¼•
            bits = samples.astype(int)
            # å¤§ç«¯åºï¼šæœ€é«˜ä½æƒé‡æœ€å¤§
            powers = 2 ** np.arange(num_pos_qubits - 1, -1, -1)
            node_idx = bits @ powers  # (shots,)
            
            # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬ (position < N)
            valid_mask = node_idx < len(neighbors)
            valid_idx = node_idx[valid_mask]
            
            # è®¡ç®—å‡»ä¸­æ¦‚ç‡ (ä»…æœ‰æ•ˆæ ·æœ¬)
            if valid_idx.size > 0:
                hits = np.count_nonzero(valid_idx == target_node)
                prob = hits / valid_idx.size
                
                if prob > best_prob:
                    best_prob = prob
                    best_time = t
                    best_hits = hits
                    best_valid_size = valid_idx.size
        
        runtime = time.time() - t0
        
        print(f"        æœç´¢æ¦‚ç‡ (hits/valid): {best_hits}/{best_valid_size} = {best_prob:.4f}")
        print(f"        ç›®æ ‡èŠ‚ç‚¹: {target_node}, æœ€ä¼˜æ—¶é—´: {best_time:.2f} (ç†è®º: {theoretical_t:.2f})")
        return float(best_prob), float(runtime), float(best_time)
    except Exception as exc:
        print(f"Search probability calculation failed: {exc}")
        return 0.0, 0.1, theoretical_t

def run_trials_for_graph(graph_data, structure_type):
    """
    å¯¹äºå•ä¸ªå›¾ï¼š
    - è®¡ç®—ç†è®ºå‡»ä¸­/è¦†ç›–æ—¶é—´ t
    - è¿è¡Œè¦†ç›–10æ¬¡ â†’ å¹³å‡ t & å¹³å‡æ—¶é—´
    - è¿è¡Œæœç´¢ä¸€æ¬¡ (SHOTS) åœ¨ç†è®ºå‡»ä¸­ t ä¸‹
    è¿”å›å…ƒç»„ (coverage_steps, coverage_time, search_probability, avg_runtime, search_steps)
    """
    graph_id = graph_data.get("graph_id", "unknown")
    neighbors = graph_data["neighbors"]
    start_node = graph_data.get("start_node", 0)
    target_node = graph_data.get("target_node")
    if target_node is None:
        target_node = get_target_node(neighbors, graph_id, structure_type)
    num_nodes = len(neighbors)
    
    print(f"        å›¾ä¿¡æ¯: {graph_id}, èŠ‚ç‚¹æ•°: {num_nodes}, èµ·å§‹: {start_node}, ç›®æ ‡: {target_node}")
    
    # ------------------- QUBIT COUNTS -------------------
    num_pos_qubits = max(1, (num_nodes - 1).bit_length())
    total_qubits = num_pos_qubits
    
    print(f"        é‡å­æ¯”ç‰¹æ•°: {num_pos_qubits} (ä½ç½®)")
    # ------------------- FALLBACK FOR HUGE GRAPHS -------------------
    if total_qubits > MAX_QUBITS or (1 << num_pos_qubits) > 1e6:
        print(f"      âš ï¸ Too many qubits ({total_qubits}) â€“ using ONLY theoretical values.")
        theory_hit = calculate_correct_theoretical_steps(graph_data, "hitting_time")
        theory_cov = calculate_correct_theoretical_steps(graph_data, "cover_time")
        return (theory_cov, 0.0, 0.0, 0.0, theory_hit )
    # ------------------- THEORETICAL TIME -------------------
    theory_hit = calculate_correct_theoretical_steps(graph_data, "hitting_time")
    theory_cov = calculate_correct_theoretical_steps(graph_data, "cover_time")
    # ------------------- COVERAGE (10Ã— average) -------------------
    coverage_t, coverage_time = calculate_coverage_steps(
        neighbors,
        start_node,
        theory_cov,
        num_pos_qubits,
        repetitions=10,
    )
    # ------------------- SEARCH (single run, SHOTS) -------------------
    search_prob, search_time, optimal_time = calculate_search_probability(
        neighbors,
        start_node,
        target_node,
        theory_hit,
        num_pos_qubits,
    )
    print(f"      ğŸ“Š Final Results:")
    print(f"          Coverage â†’ avg {coverage_t:.2f} t (theory {theory_cov:.2f})")
    print(f"          Search    â†’ prob {search_prob:.4f} at {optimal_time:.2f} t (time {search_time:.3f}s)")
    return ( round(coverage_t, 2), float(coverage_time), float(search_prob), float(search_time), round(optimal_time, 2) )

# 3. æ¨¡æ‹Ÿæ‰§è¡Œä¸å¯è§†åŒ–ï¼ˆç»“æœä¿å­˜ä¸ºCSVï¼Œæ— é¢å¤–å¯è§†åŒ–ï¼ŒæŒ‰æ¡†æ¶ï¼‰
def main():
    """ä¸»å‡½æ•°ï¼ˆä¸DTQWç›¸åŒï¼‰"""
    algo_results_path = os.path.join(RESULTS_PATH, f'results-{ALGORITHM_NAME}')
    os.makedirs(algo_results_path, exist_ok=True)
    structure_types = ['Tree', 'Grid', 'SmallWorld', 'GluedTree', 'Hypercube']
    all_results = []
    total_start_time = time.time()
    for struct_type in structure_types:
        print(f"\n{'=' * 80}")
        print(f"ğŸ“ Processing Structure Type: {struct_type}")
        print(f"{'=' * 80}")
        dataset_dir = os.path.join(DATASETS_PATH, f'datasets-{struct_type.lower()}s')
        dataset_file = os.path.join(dataset_dir, f'{struct_type.lower()}s_all_variants_quantum.json')
        if not os.path.exists(dataset_file):
            print(f"âŒ Dataset not found for {struct_type}: {dataset_file}")
            continue
        try:
            with open(dataset_file, 'r') as f:
                all_graphs_data = json.load(f)
            if not isinstance(all_graphs_data, list):
                all_graphs_data = [all_graphs_data]
            print(f"ğŸ“ˆ Found {len(all_graphs_data)} graphs for {struct_type}.")
            for idx, graph_data in enumerate(all_graphs_data):
                print(f"\nğŸ“Š Processing Graph {idx + 1}/{len(all_graphs_data)}: {graph_data.get('graph_id', 'N/A')}")
                try:
                    if 'neighbors' not in graph_data or not graph_data['neighbors']:
                        print("  Skipping graph with no neighbors.")
                        continue
                    if 'start_node' not in graph_data:
                        graph_data['start_node'] = 0
                    if 'target_node' not in graph_data:
                        graph_data['target_node'] = get_target_node(
                            graph_data['neighbors'], graph_data.get('graph_id', ''), struct_type
                        )
                    # è¿è¡Œåˆ†æ
                    coverage_steps, coverage_time, search_probability, search_time, search_steps = run_trials_for_graph(
                        graph_data, struct_type
                    )
                    # æ„å»ºç»“æœè®°å½•
                    result = {
                        'æ•°æ®åç§°': graph_data.get('graph_id', f'{struct_type}_{idx}'),
                        'ç±»å‹': struct_type,
                        'coverage_steps': coverage_steps,
                        'coverage_time': coverage_time,
                        'search_probability': search_probability,
                        'search_time': search_time,
                        'search_steps': search_steps
                    }
                    all_results.append(result)
                except Exception as e:
                    print(f"  âŒ Critical error processing graph {graph_data.get('graph_id', 'N/A')}: {e}")
        except Exception as e:
            print(f"âŒ Fatal error loading or processing dataset for {struct_type}: {e}")
    # ä¿å­˜ç»“æœ
    if all_results:
        output_file = os.path.join(algo_results_path, 'CTQW_Analysis_Results.csv')
        try:
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=['æ•°æ®åç§°', 'ç±»å‹', 'coverage_steps', 'coverage_time', 'search_probability', 'search_time', 'search_steps'])
                writer.writeheader()
                writer.writerows(all_results)
            total_time = time.time() - total_start_time
            print(f"\nğŸ‰ CTQW ANALYSIS COMPLETED!")
            print(f"â±ï¸  Total time: {total_time:.1f}s")
            print(f"ğŸ“ Results saved to: {output_file}")
            print(f"ğŸ“Š Processed {len(all_results)} graphs total")
            # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
            if all_results:
                avg_search_prob = np.mean([r['search_probability'] for r in all_results])
                high_prob_count = sum(1 for r in all_results if r['search_probability'] > 0.1)
                print(f"\nğŸ“ˆ Key Statistics:")
                print(f"   Average Search Probability: {avg_search_prob:.4f}")
                print(f"   Graphs with >10% search probability: {high_prob_count}/{len(all_results)}")
        except Exception as e:
            print(f"âŒ Error saving results to CSV: {e}")
    else:
        print(f"âŒ No results generated!")

if __name__ == '__main__':
    random.seed(42)
    np.random.seed(42)
    main()